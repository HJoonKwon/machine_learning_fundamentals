{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HJoonKwon/ml_fundamentals/blob/main/NaiveBayes_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGRwEMw9_73Q"
   },
   "source": [
    "## Naive Bayes Algorithm \n",
    "\n",
    "- Supervised learning (We need training data) \n",
    "- For classification \n",
    "- Based on Bayes' Theorem\n",
    "- \"Naive\" means we assume features are independent to each other. \n",
    "\n",
    "### How does it work?\n",
    "- Based on Bayes' theorm, we can calculate posterior probability using prior proabibility, likelihood, and evidence(or marginal probability).\n",
    " $$P(A|B) =  \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "- We can just apply the Bayes' theorem to prediction for the probability of the output(classification or regression). Let's assume that the training data has ```n``` number of features, and we want to predict the probability of ```y``` given ```X```. Then,\n",
    " $$P(y|X) = P(y|x_1, ..., x_n) = \\frac{P(x_1, .., x_n|y)P(y)}{P(x_1, ..., x_n)}  = \\frac{P(x_1|y)...P(x_n|y)P(y)}{P(x_1)...P(x_n)}$$ \n",
    "- The process above includes multiplying fraction of number multiple times, which can cause underflow in numerical calcuation. It is better to wrap the multiplication process with log operation to avoid underflow. \n",
    " $$log(P(y|X) = [\\sum_{i=1}^{n} log(P(x_i|y))] + log(P(y)) - [\\sum_{i=1}^n log(P(x_i))] $$\n",
    "- For each ```y``` value, we can calculate ```P(y|X)``` and find the ```y``` that makes ```P(y|X)``` the largest. We can make the maximization process simpler by removing the denominator(or evidence) in the equation above because our predicted ```y``` does not affect the denominator. \n",
    "\n",
    " ### What kind of data can it handle?\n",
    " - Continuous (Gaussian Naive Bayes)\n",
    " - Discrete (Binary/Multinomial Naive Bayes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "NSx5I2LAHWjg"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oHQ9FFM_rzjY"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_algorithms.naive_bayes import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pf4MoTgG82K"
   },
   "source": [
    "### 1) Prepare for the dataset \n",
    "- We are going to use the breast cancer dataset provided by scikit-learn. \n",
    "- We can see that all features are continuous, so the Gaussian Naive Bayes would be our choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dV9ac0Xm88N5"
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTXqDvrsHlIV",
    "outputId": "79165144-7632-4aca-d313-9f9ac05eb90e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      " 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      " 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      " 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      " 4.601e-01 1.189e-01]\n",
      "['malignant' 'benign']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(data['data'].shape)\n",
    "print(data['feature_names'])\n",
    "print(data['data'][0])\n",
    "print(data['target_names'])\n",
    "print(data['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ByagjEILiBe"
   },
   "source": [
    "### 2) Preprocessing the data\n",
    "- Scale Data (Normalization)\n",
    "- Split Data into train/test sets  \n",
    "- Display data in DataFrame for better understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-jhcW00gKbot",
    "outputId": "192faffc-d61f-4f30-b93e-51c3efeddec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.09706398 -2.07333501  1.26993369  0.9843749   1.56846633  3.28351467\n",
      "  2.65287398  2.53247522  2.21751501  2.25574689  2.48973393 -0.56526506\n",
      "  2.83303087  2.48757756 -0.21400165  1.31686157  0.72402616  0.66081994\n",
      "  1.14875667  0.90708308  1.88668963 -1.35929347  2.30360062  2.00123749\n",
      "  1.30768627  2.61666502  2.10952635  2.29607613  2.75062224  1.93701461]\n",
      "train_X: (455, 30)\n",
      "test_X: (114, 30)\n",
      "train_y: (455,)\n",
      "test_y: (114,)\n"
     ]
    }
   ],
   "source": [
    "X = data['data']\n",
    "y = data['target']\n",
    "X = normalize(X)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X[0])\n",
    "print(f'train_X: {train_X.shape}')\n",
    "print(f'test_X: {test_X.shape}')\n",
    "print(f'train_y: {train_y.shape}')\n",
    "print(f'test_y: {test_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfDgmAo1Lr-j"
   },
   "source": [
    "### 3) Gaussian Naive Bayes Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GaussianNaiveBayes():\n",
      "\n",
      "    def __init__(self):\n",
      "        self.train_X: np.ndarray\n",
      "        self.train_Y: np.ndarray\n",
      "        self.classes: np.ndarray\n",
      "\n",
      "    def log_likelihood(self, X: np.ndarray) -> np.ndarray:\n",
      "\n",
      "        # X: (m x n)\n",
      "        # log(P(X|Y))\n",
      "        means, stds = mean_and_std(self.train_X, self.train_Y)\n",
      "        log_likelihood = np.zeros((means.shape[0], X.shape[0]))\n",
      "        for i in range(means.shape[0]):\n",
      "            likelihood = 1 / np.sqrt(2 * np.pi) / stds[i] * np.exp(\n",
      "                -0.5 * np.square((X - means[i]) / stds[i]))\n",
      "            log_likelihood[i] = np.sum(np.log(likelihood),\n",
      "                                       axis=1).reshape(1, -1)\n",
      "        return log_likelihood\n",
      "\n",
      "    def log_priors(self):\n",
      "        # log(P(Y))\n",
      "        priors = np.zeros(self.classes.shape)\n",
      "        for cls in self.classes:\n",
      "            priors[cls] = np.count_nonzero(self.train_Y == cls) / len(\n",
      "                self.train_Y)\n",
      "        return np.log(priors).reshape(-1, 1)\n",
      "\n",
      "    def log_scores(self, X: np.ndarray):\n",
      "        # X: (m x n)\n",
      "        # scores = posterior * evidence (we don't need to calculate evidence)\n",
      "        log_priors = self.log_priors()\n",
      "        log_likelihood = self.log_likelihood(X)\n",
      "        log_scores = log_priors + log_likelihood\n",
      "        return log_scores\n",
      "\n",
      "    def fit(self, X: np.ndarray, Y: np.ndarray) -> None:\n",
      "        self.train_X = X\n",
      "        self.train_Y = Y\n",
      "        self.classes = np.unique(self.train_Y)\n",
      "\n",
      "    def predict(self, X: np.ndarray):\n",
      "        log_scores = self.log_scores(X)\n",
      "        preds = np.argmax(log_scores, axis=0)\n",
      "        return preds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "lines = inspect.getsource(GaussianNaiveBayes)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53CUrSPTfR4P"
   },
   "source": [
    "### 4) Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "nWvcuVfAhPLi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is: 96.49122807017544 %\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNaiveBayes()\n",
    "gnb.fit(train_X, train_y.reshape(-1, 1))\n",
    "my_preds = gnb.predict(test_X)\n",
    "my_accuracy = np.sum(my_preds==test_y)/test_y.shape[0]\n",
    "print(f\"Test accuracy is: {my_accuracy*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Validation \n",
    "- check it the implemented model works the same as the Gaussian Naive-Bayes model in scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is: 96.49122807017544 %\n"
     ]
    }
   ],
   "source": [
    "validate_model = GaussianNB()\n",
    "validate_model.fit(train_X, train_y)\n",
    "preds = validate_model.predict(test_X)\n",
    "accuracy = np.sum(preds==test_y)/test_y.shape[0]\n",
    "print(f\"Test accuracy is: {accuracy*100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(my_accuracy, accuracy)\n",
    "assert np.sum(my_preds == preds)/len(my_preds) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "did_kqUbCDI_"
   },
   "source": [
    "## References \n",
    "- https://medium.com/@rangavamsi5/na%C3%AFve-bayes-algorithm-implementation-from-scratch-in-python-7b2cc39268b9\n",
    "- https://towardsdatascience.com/implementing-naive-bayes-algorithm-from-scratch-python-c6880cfc9c41"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNTZEAbRfZFTGI9irjgicd0",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
